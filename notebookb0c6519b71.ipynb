{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/RVC-Boss/GPT-SoVITS.git\n%cd GPT-SoVITS\n!apt-get update && apt-get install -y --no-install-recommends tzdata ffmpeg libsox-dev parallel aria2 git git-lfs && git lfs install\n!pip install -r requirements.txt","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# @title Download pretrained models 下载预训练模型\n!mkdir -p /kaggle/working/GPT-SoVITS/GPT_SoVITS/pretrained_models\n!mkdir -p /kaggle/working/GPT-SoVITS/tools/asr/models\n!mkdir -p /kaggle/working/GPT-SoVITS/tools/uvr5\n%cd /kaggle/working/GPT-SoVITS/GPT_SoVITS/pretrained_models\n!git clone https://huggingface.co/lj1995/GPT-SoVITS\n%cd /kaggle/working/GPT-SoVITS/tools/asr/models\n!git clone https://www.modelscope.cn/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch.git\n!git clone https://www.modelscope.cn/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch.git\n!git clone https://www.modelscope.cn/damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch.git\n# # @title UVR5 pretrains 安装uvr5模型\n%cd /kaggle/working/GPT-SoVITS/tools/uvr5\n!git clone https://huggingface.co/Delik/uvr5_weights\n!git config core.sparseCheckout true\n!mv /kaggle/working/GPT-SoVITS/GPT_SoVITS/pretrained_models/GPT-SoVITS/* /kaggle/working/GPT-SoVITS/GPT_SoVITS/pretrained_models/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# @title launch WebUI 启动WebUI\n%cd /kaggle/working/GPT-SoVITS/\n!mkdir input\n!cp /kaggle/input/cliiford/Cliffford-GTAO-Quote.ogg /kaggle/working/GPT-SoVITS/input\n!npm install -g localtunnel\nimport subprocess\nimport threading\nimport time\nimport socket\nimport urllib.request\ndef iframe_thread(port):\n    while True:\n        time.sleep(0.5)\n        sock= socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        result = sock.connect_ex(('127.0.0.1', port))\n        if result == 0:\n            break\n        sock.close()\n        from colorama import Fore, Style\n    print (Fore.GREEN + \"\\nIP: \", Fore. RED, urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"), \"\\n\", Style. RESET_ALL)\n    p = subprocess.Popen([\"lt\", \"--port\", \"{}\".format(port)], stdout=subprocess.PIPE)\n    for line in p.stdout:\n        print(line.decode(), end='')\nthreading.Thread (target=iframe_thread, daemon=True, args=(9874,)).start()\n!python  webui.py","metadata":{"scrolled":true,"_kg_hide-output":true,"execution":{"iopub.status.idle":"2024-05-08T15:31:03.832894Z","shell.execute_reply.started":"2024-05-08T14:54:53.271215Z","shell.execute_reply":"2024-05-08T15:31:03.831534Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"\"/opt/conda/bin/python\" GPT_SoVITS/s2_train.py --config \"/kaggle/working/GPT-SoVITS/TEMP/tmp_s2.json\"\n2024-05-08 15:25:26.085733: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-08 15:25:26.085788: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-08 15:25:26.087939: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-05-08 15:25:33.051555: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-08 15:25:33.051609: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-08 15:25:33.052888: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-05-08 15:25:33.129261: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-08 15:25:33.129312: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-08 15:25:33.131335: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nphoneme_data_len: 892\nwav_data_len: 892\n100%|██████████████████████████████████████| 892/892 [00:00<00:00, 52572.46it/s]\nskipped_phone:  0 , skipped_dur:  3\ntotal left:  889\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\nphoneme_data_len: 892\nwav_data_len: 892\n100%|██████████████████████████████████████| 892/892 [00:00<00:00, 63705.88it/s]\nskipped_phone:  0 , skipped_dur:  3\ntotal left:  889\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n/opt/conda/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\nlogs/Nahinta/logs_s2/D_233333333333.pth\nlogs/Nahinta/logs_s2/D_233333333333.pth\nload \nload \nlogs/Nahinta/logs_s2/G_233333333333.pth\nlogs/Nahinta/logs_s2/G_233333333333.pth\nload \nload \n/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n2024-05-08 15:25:46.206652: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-08 15:25:46.206724: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-08 15:25:46.208961: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-05-08 15:25:46.306995: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-08 15:25:46.307049: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-08 15:25:46.308892: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-05-08 15:25:53.111955: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-08 15:25:53.112013: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-08 15:25:53.113990: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-05-08 15:25:53.247273: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-08 15:25:53.247321: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-08 15:25:53.248748: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-05-08 15:26:00.052881: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-08 15:26:00.052950: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-08 15:26:00.054266: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-05-08 15:26:00.065283: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-08 15:26:00.065330: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-08 15:26:00.066638: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-05-08 15:26:07.047820: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-08 15:26:07.047881: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-08 15:26:07.049292: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-05-08 15:26:07.104783: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-08 15:26:07.104840: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-08 15:26:07.106896: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-05-08 15:26:13.955484: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-08 15:26:13.955558: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-08 15:26:13.956852: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-05-08 15:26:14.087430: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-08 15:26:14.087477: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-08 15:26:14.089501: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-05-08 15:26:21.031038: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-08 15:26:21.031096: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-08 15:26:21.032368: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-05-08 15:26:21.149174: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-08 15:26:21.149228: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-08 15:26:21.150646: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n0it [00:00, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/functional.py:650: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\nNote: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/SpectralOps.cpp:863.)\n  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/functional.py:650: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\nNote: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/SpectralOps.cpp:863.)\n  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/functional.py:650: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\nNote: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/SpectralOps.cpp:863.)\n  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/functional.py:650: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\nNote: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/SpectralOps.cpp:863.)\n  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/functional.py:650: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\nNote: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/SpectralOps.cpp:863.)\n  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/functional.py:650: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\nNote: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/SpectralOps.cpp:863.)\n  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n0it [00:00, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/functional.py:650: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\nNote: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/SpectralOps.cpp:863.)\n  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/functional.py:650: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\nNote: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/SpectralOps.cpp:863.)\n  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/functional.py:650: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\nNote: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/SpectralOps.cpp:863.)\n  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/functional.py:650: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\nNote: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/SpectralOps.cpp:863.)\n  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/functional.py:650: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\nNote: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/SpectralOps.cpp:863.)\n  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/functional.py:650: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\nNote: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/SpectralOps.cpp:863.)\n  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/functional.py:650: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\nNote: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/SpectralOps.cpp:863.)\n  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/functional.py:650: UserWarning: ComplexHalf support is experimental and many operators don't support it yet. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/EmptyTensor.cpp:31.)\n  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/functional.py:650: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\nNote: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/SpectralOps.cpp:863.)\n  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/functional.py:650: UserWarning: ComplexHalf support is experimental and many operators don't support it yet. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/EmptyTensor.cpp:31.)\n  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\ngrad.sizes() = [1, 9, 96], strides() = [41760, 96, 1]\nbucket_view.sizes() = [1, 9, 96], strides() = [864, 96, 1] (Triggered internally at /usr/local/src/pytorch/torch/csrc/distributed/c10d/reducer.cpp:320.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\ngrad.sizes() = [1, 9, 96], strides() = [42912, 96, 1]\nbucket_view.sizes() = [1, 9, 96], strides() = [864, 96, 1] (Triggered internally at /usr/local/src/pytorch/torch/csrc/distributed/c10d/reducer.cpp:320.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n65it [01:28,  1.36s/it]\n65it [01:27,  1.35s/it]\n\"/opt/conda/bin/python\" GPT_SoVITS/inference_webui.py\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package cmudict to /usr/share/nltk_data...\n[nltk_data]   Package cmudict is already up-to-date!\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of the model checkpoint at GPT_SoVITS/pretrained_models/chinese-hubert-base were not used when initializing HubertModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n- This IS expected if you are initializing HubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing HubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of HubertModel were not initialized from the model checkpoint at GPT_SoVITS/pretrained_models/chinese-hubert-base and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n<All keys matched successfully>\nNumber of parameter: 77.49M\nIMPORTANT: You are using gradio version 3.38.0, however version 4.29.0 is available, please upgrade.\n--------\nRunning on local URL:  http://0.0.0.0:9872\n\"/opt/conda/bin/python\" GPT_SoVITS/inference_webui.py\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package cmudict to /usr/share/nltk_data...\n[nltk_data]   Package cmudict is already up-to-date!\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of the model checkpoint at GPT_SoVITS/pretrained_models/chinese-hubert-base were not used when initializing HubertModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n- This IS expected if you are initializing HubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing HubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of HubertModel were not initialized from the model checkpoint at GPT_SoVITS/pretrained_models/chinese-hubert-base and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n<All keys matched successfully>\nNumber of parameter: 77.49M\nIMPORTANT: You are using gradio version 3.38.0, however version 4.29.0 is available, please upgrade.\n--------\nRunning on local URL:  http://0.0.0.0:9872\n^C\nKeyboard interruption in main thread... closing server.\nKeyboard interruption in main thread... closing server.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/GPT-SoVITS/\n!npm install -g localtunnel\nimport subprocess\nimport threading\nimport time\nimport socket\nimport urllib.request\ndef iframe_thread(port):\n    while True:\n        time.sleep(0.5)\n        sock= socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        result = sock.connect_ex(('127.0.0.1', port))\n        if result == 0:\n            break\n        sock.close()\n        from colorama import Fore, Style\n    print (Fore.GREEN + \"\\nIP: \", Fore. RED, urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"), \"\\n\", Style. RESET_ALL)\n    p = subprocess.Popen([\"lt\", \"--port\", \"{}\".format(port)], stdout=subprocess.PIPE)\n    for line in p.stdout:\n        print(line.decode(), end='')\nthreading.Thread (target=iframe_thread, daemon=True, args=(9872,)).start()\n!python  ./GPT_SoVITS/inference_webui.py","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/GPT-SoVITS/\n!zip GPT_packed.zip /kaggle/working/GPT-SoVITS/SoVITS_weights/* /kaggle/working/GPT-SoVITS/GPT_weights/*","metadata":{"execution":{"iopub.status.busy":"2024-05-08T15:33:11.280814Z","iopub.execute_input":"2024-05-08T15:33:11.281811Z","iopub.status.idle":"2024-05-08T15:33:46.771888Z","shell.execute_reply.started":"2024-05-08T15:33:11.281768Z","shell.execute_reply":"2024-05-08T15:33:46.770879Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"/kaggle/working/GPT-SoVITS\n  adding: kaggle/working/GPT-SoVITS/SoVITS_weights/Nahinta_e4_s512.pth (deflated 8%)\n  adding: kaggle/working/GPT-SoVITS/SoVITS_weights/Nahinta_e8_s1024.pth (deflated 8%)\n  adding: kaggle/working/GPT-SoVITS/SoVITS_weights/Nahinta_e8_s520.pth (deflated 8%)\n  adding: kaggle/working/GPT-SoVITS/GPT_weights/Nahinta-e10.ckpt (deflated 8%)\n  adding: kaggle/working/GPT-SoVITS/GPT_weights/Nahinta-e15.ckpt (deflated 8%)\n  adding: kaggle/working/GPT-SoVITS/GPT_weights/Nahinta-e5.ckpt (deflated 8%)\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/GPT-SoVITS/input\n!rm -rf 2077Johnny-SoVITS\n!git clone https://github.com/Separatee/2077Johnny-SoVITS/\n%mv ./2077Johnny-SoVITS/audio.mp3 ./\n!rm -rf ./2077Johnny-SoVITS","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/GPT-SoVITS/GPT_SoVITS\n!/opt/conda/bin/python inference_webui.py","metadata":{"execution":{"iopub.status.busy":"2024-05-08T15:32:35.194530Z","iopub.execute_input":"2024-05-08T15:32:35.194907Z","iopub.status.idle":"2024-05-08T15:32:54.769153Z","shell.execute_reply.started":"2024-05-08T15:32:35.194866Z","shell.execute_reply":"2024-05-08T15:32:54.768053Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"/kaggle/working/GPT-SoVITS/GPT_SoVITS\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package cmudict to /usr/share/nltk_data...\n[nltk_data]   Package cmudict is already up-to-date!\nTraceback (most recent call last):\n  File \"/kaggle/working/GPT-SoVITS/GPT_SoVITS/inference_webui.py\", line 70, in <module>\n    i18n = I18nAuto()\n  File \"/kaggle/working/GPT-SoVITS/tools/i18n/i18n.py\", line 21, in __init__\n    self.language_map = load_language_list(language)\n  File \"/kaggle/working/GPT-SoVITS/tools/i18n/i18n.py\", line 7, in load_language_list\n    with open(f\"./i18n/locale/{language}.json\", \"r\", encoding=\"utf-8\") as f:\nFileNotFoundError: [Errno 2] No such file or directory: './i18n/locale/en_US.json'\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.chdir('/kaggle/working')\nprint(os.getcwd())\nprint(os.listdir(\"/kaggle/working\"))\nfrom IPython.display import Filelink\nfl('GPT_SoVITS/GPT_packed.zip')","metadata":{"execution":{"iopub.status.busy":"2024-05-08T15:40:36.990440Z","iopub.execute_input":"2024-05-08T15:40:36.991377Z","iopub.status.idle":"2024-05-08T15:40:37.022915Z","shell.execute_reply.started":"2024-05-08T15:40:36.991319Z","shell.execute_reply":"2024-05-08T15:40:37.021583Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"/kaggle/working\n['.virtual_documents', 'GPT-SoVITS']\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mgetcwd())\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/working\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Filelink \u001b[38;5;28;01mas\u001b[39;00m fl\n\u001b[1;32m      6\u001b[0m fl(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPT_SoVITS/GPT_packed.zip\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'Filelink' from 'IPython.display' (/opt/conda/lib/python3.10/site-packages/IPython/display.py)"],"ename":"ImportError","evalue":"cannot import name 'Filelink' from 'IPython.display' (/opt/conda/lib/python3.10/site-packages/IPython/display.py)","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}